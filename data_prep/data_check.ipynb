{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/julia/DATA/Programs/miniconda3/envs/nlp39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.chdir(os.path.abspath('..'))\n",
    "\n",
    "from data_prep.data_utils import load_help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 17:14:49.061 | INFO     | data_prep.data_utils:load_help:52 - Index(['sentence1', 'sentence2', 'gold_label', 'gold_label_num'], dtype='object')\n",
      "2023-04-09 17:14:49.164 | INFO     | data_prep.data_utils:load_help:56 - (35891, 4)\n",
      "100%|██████████| 32/32 [00:00<00:00, 257.08ba/s]\n",
      "Casting the dataset: 100%|██████████| 32/32 [00:00<00:00, 644.14ba/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 835.95ba/s]\n",
      "Casting the dataset: 100%|██████████| 5/5 [00:00<00:00, 824.45ba/s]\n"
     ]
    }
   ],
   "source": [
    "# Load HELP_TRAIN and HELP_TEST\n",
    "\n",
    "train, test = load_help()\n",
    "train_df = pd.DataFrame(train)\n",
    "test_df = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': Value(dtype='string', id=None),\n",
       " 'hypothesis': Value(dtype='string', id=None),\n",
       " 'gold_label': Value(dtype='string', id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None),\n",
       " 'label': ClassLabel(names=['entailment', 'neutral'], id=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch original help dataset for extra labels\n",
    "help_original = pd.read_csv('data/HELP/HELP_original.tsv', sep='\\t', index_col=0)\n",
    "help_original['premise']  = help_original['ori_sentence']\n",
    "help_original['hypothesis']  = help_original['new_sentence']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join original labels to our HELP splits\n",
    "test_df = pd.merge(test_df, help_original, on=['premise', 'hypothesis'], how='left', indicator='Exist')\n",
    "test_df['Exist'] = np.where(test_df.Exist==\"both\", True, False)\n",
    "test_df = test_df.loc[test_df.Exist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.merge(train_df, help_original, on=['premise', 'hypothesis'], how='left', indicator='Exist')\n",
    "train_df['Exist'] = np.where(train_df.Exist==\"both\", True, False)\n",
    "train_df = train_df.loc[train_df.Exist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "downward_monotone    17909\n",
       "upward_monotone       7046\n",
       "conjunction           6044\n",
       "non_monotone           992\n",
       "disjunction            342\n",
       "Name: monotonicity, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.monotonicity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_context(row):\n",
    "    # the replace target/source is not consistent with premise/hypothesis, so we try both and take the successful one:\n",
    "    premise = row['premise']\n",
    "    try:\n",
    "        context = premise.replace(row['replace_source'] ,'x')\n",
    "    except:\n",
    "        context = None\n",
    "    try:\n",
    "        check_context = premise.replace(row['replace_target'] ,'x')\n",
    "    except:\n",
    "        check_context = None\n",
    "\n",
    "    if context and context != premise: \n",
    "        return context\n",
    "    elif check_context and check_context != premise:\n",
    "        return check_context\n",
    "    else: \n",
    "        print('Logging an ignored example:')\n",
    "        print(premise)\n",
    "        print(row['replace_source'])\n",
    "        print(row['replace_target'])\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_extract_contexts(df, split_name, sample_size, seed):\n",
    "    sample_for_downward_contexts = df.loc[df.monotonicity=='downward_monotone'].sample(sample_size, random_state=seed)\n",
    "    sample_for_downward_contexts\n",
    "    downward_contexts = pd.DataFrame(columns=['context', 'monotonicity', 'determiner', 'replace_target', 'replace_source', 'replace_mode', 'label'])\n",
    "    downward_contexts['context'] = sample_for_downward_contexts.apply(extract_context, axis=1).dropna()\n",
    "    downward_contexts['monotonicity'] = 'down'\n",
    "    downward_contexts\n",
    "\n",
    "    sample_for_upward_contexts = df.loc[df.monotonicity=='upward_monotone'].sample(sample_size, random_state=seed)\n",
    "    sample_for_upward_contexts = sample_for_upward_contexts.dropna()\n",
    "    upward_contexts = pd.DataFrame(columns=['context', 'monotonicity'])\n",
    "    upward_contexts['context'] = sample_for_upward_contexts.apply(extract_context, axis=1).dropna()\n",
    "    upward_contexts['monotonicity'] = 'up'\n",
    "\n",
    "    # combine full contexts split\n",
    "    help_contexts = pd.concat([downward_contexts, upward_contexts])\n",
    "    with open(f'data/HELP_Contexts/help_contexts_{split_name}.tsv', 'w+') as file:\n",
    "        file.write(help_contexts.to_csv(sep='\\t'))\n",
    "\n",
    "    with open(f'data/HELP_Contexts/{split_name}_sample_for_upward_contexts.tsv', 'w+') as file:\n",
    "        file.write(sample_for_upward_contexts.to_csv(sep='\\t'))\n",
    "    with open(f'data/HELP_Contexts/{split_name}_sample_for_downward_contexts.tsv', 'w+') as file:\n",
    "        file.write(sample_for_downward_contexts.to_csv(sep='\\t'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging an ignored example:\n",
      "Tom did n't reply to any of Mary 's letter .\n",
      "any letter\n",
      "any of Mary s letter\n",
      "Logging an ignored example:\n",
      "There is no need to reply to that x .\n",
      "x\n",
      "letter\n"
     ]
    }
   ],
   "source": [
    "seed=11\n",
    "sample_and_extract_contexts(train_df, 'train', 500, seed)\n",
    "sample_and_extract_contexts(test_df, 'test', 300, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5631747dc3ad6eb60a3da5f4cdcebe149d51d3449295229bf66075ef01b5a217"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
